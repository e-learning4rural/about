 

*An international showcase of ethical, community-evaluated AI solutions for remote and marginalized education contexts*


## üåü What this symposium is about

The Inaugural Symposium on Ethical AI Solutions for Remote Learning is a new online initiative that advances educational equity by centering the perspectives of rural and marginalized communities in the evaluation of AI-driven learning tools. While other AI education symposiums exist, few meaningfully involve end-users from under-resourced contexts in the evaluation process. This symposium fills that gap by using a participatory model that integrates developers, rural educators, and remote learners into a structured, compensated evaluation framework.

The program features two tracks: **Real-World Impact Track** and **Innovation Track**.

Submissions made to both tracks will undergo structured testing by both expert educators from rural settings and remote learners from marginalized communities. Feedback from both panels will directly contribute to scoring, based on a transparent rubric that emphasizes technical innovation, cultural sensitivity, usability, ethical design, and scalability.

By embedding end-user perspectives into the evaluation process, the symposium pilots a replicable model for participatory, ethically grounded AI assessment. This initiative democratizes AI education innovation, elevates community expertise, and provides student developers with substantive, real-world validation. All materials, evaluation criteria, and recorded presentations will be made publicly available to maximize the broader impact.

## üèÜ Competition Tracks

| Track | Goals | Awards |
| -- | -- | -- |
| T1: üå± Real-World Impact | For solutions ready (or near-ready) for deployment in under-resourced contexts | Winner: $1000 <br> Runner-up: $300 |
| T2: üí° Innovation |  For novel, experimental, or breakthrough approaches advancing ethical AI in remote learning | Winner: $700 <br> Runner-up: $200 |

## How it works?
- Each participant may submit no more than one submissions
- A submission is consider complete if it contains a functional prototype, technical documentation, and video presentation
- The panels will prescreen all *completed* submissions based on the published evaluation rubric (see below)
- The top three submissions from each track will be selected by the panels, based on the prescreening scores
- Finalists will receive structured feedback through usability testing with educators from rural contexts and remote learners from marginalized communities. Feedback will be collected via surveys and facilitated discussions, and it will contribute to the final score
- Non-finalists are also invited to present their videos and answer questions during the showcase and will have the chance to be selected by the public as the 'Public's Favorite'
  
--- 

## Program Timeline

| Phase | Timeline | Tasks |
| :-- | :-- | :-- |
| 1. Submission üöÄ | May 6 ‚Äì July 6 | Participants submit: <br> - üé• 5‚Äì7 minute video demonstration <br> - üìÑ Technical + ethical documentation <br> - üß™ Functional prototype* | 
| 2. Participatory Prototype Evaluation üîé | July 7 ‚Äì July 28 | All projects undergo structured, community-informed evaluation |
| 3. Live Online Showcase & final judging üé§ | August 15 | Four-hour online symposium: <br> - Finalist presentations  <br> - Moderated Q&A  <br> - Reflections from educator and learner panelists <br>  - Community voting <br>  - Awards ceremony  |

*Examples of prototypes: web app, mobile app, lightweight executable, or guided simulation hosted on an open repository 

### Panel 1: Expert educators
Educators from rural or under-resourced contexts evaluate:
- Feasibility  
- Pedagogical value  
- Cultural appropriateness  
- Ethical design  

### Panel 2: Remote learners 
Learners from marginalized communities participate in a 14-day guided testing period. They assess:
- Usability  
- Accessibility in low-bandwidth settings  
- Cultural resonance  
- Relevance to lived learning needs  

Participation includes:
- 2‚Äì3 hours total engagement  
- Structured surveys  
- Facilitated feedback sessions  
- Fair compensation  
- Privacy and risk safeguards  

## Scoring rubric
*click to expand each category*

### Track 1: Real-world impact 

<details>
<summary> Needs Alignment (20%)</summary>
Description: Does the AI tool directly address the specific needs of marginalized or rural learners, particularly in remote and under-resourced contexts?

Questions to consider:
1. Does the tool solve a clearly identified problem in these communities (e.g., access to resources, low bandwidth, cultural relevance)?
2. How well does the solution align with the real-world needs of the target user group?
3. Has the project incorporated user feedback from remote learners and educators during the design phase?
4. Is there evidence of how users will be motivated to use the tool regularly (e.g., gamification, ease of use, feedback loops)?
</details>


<details>
<summary> Ethical Design (40%)</summary>
 
Description: Does the AI solution address potential biases, ensuring that it does not disadvantage certain groups, particularly marginalized communities?

Questions to consider:
### Fairness and Bias Mitigation
- Does the tool account for diverse learners and their needs (e.g., language, cultural backgrounds, socio-economic status)?
- How does the system identify and mitigate potential bias in its algorithms and data usage?
- Is there any evidence of fairness testing (e.g., does the tool work equally well for learners from different backgrounds)?
### Transparency & Accountability
- Does the tool provide clear explanations about its algorithms, data sources, and how decisions are made?
- Are the creators accountable for potential harm caused by the tool, and is there a process for addressing user concerns?
- Is there a clear and transparent methodology for how the tool was designed, tested, and iterated?
### Data Privacy & Security
- Does the tool follow ethical data collection practices, such as informed consent, data anonymization, and user control over their data?
- How does the tool ensure security for vulnerable populations, protecting them from data breaches or misuse?
- Does the tool comply with relevant data protection laws and local regulations for remote learners?
### Cultural Sensitivity and Inclusivity    
- Is the tool designed with cultural sensitivity in mind, addressing the unique needs and backgrounds of remote learners from diverse communities?
- How inclusive is the solution? Does it consider disabilities, language barriers, and regional diversity in design?
- Has the tool been tested with target communities to ensure cultural appropriateness?
</details>

<details>
<summary> Usability & Accessibility (40%)</summary>
Description: How intuitive and easy-to-use is the AI tool for learners, especially those with limited technical experience? Does the interface design facilitate smooth interaction?

Questions to consider:
1. Is the interface clear, intuitive, and easy to navigate without prior training or significant guidance?
2. Does the tool provide easy-to-understand instructions and feedback during use?
3. Is there a balance between simplicity (for accessibility) and functionality (for depth)?

Learners will be given a survey questions, e.g.:
- ‚ÄúI could use this tool without external help.‚Äù (1‚Äì5 scale)
- ‚ÄúThe tool worked well with limited internet.‚Äù (1‚Äì5 scale)
- ‚ÄúInstructions were clear.‚Äù (1‚Äì5 scale)
- ‚ÄúI would use this for my own learning.‚Äù (Yes/No + Why)

Educators then provide evaluation based on:
- Learners' ratings
- Thematic summary of learners' qualitative responses
- Flagged issues (e.g., confusion, harm concerns)
</details>


### Track 2: Innovation Track


<details>
<summary> Needs Alignment (20%)</summary>
Same as those listed in Track 1
</details>


<details>
<summary> Ethical Design (40%)</summary>
Same as those listed in Track 1
</details>

<details>
<summary> Originality (40%)</summary>
Description: How original and unique is the solution? Does it introduce a new concept, approach, or technology, or does it build on existing ideas in a groundbreaking way?

Questions to consider:
- Does the solution introduce a novel idea or approach to solving the problem?
- How does it differ from or improve upon existing solutions in the field?
- Does it leverage new or emerging technologies in a creative way?
 
</details>

