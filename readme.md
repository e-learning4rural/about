 

*An international showcase of ethical, community-evaluated AI solutions for remote and marginalized education contexts*


## üåü What this symposium is about

The Inaugural Symposium on Ethical AI Solutions for Remote Learning is a new online initiative that advances educational equity by centering the perspectives of rural and marginalized communities in the evaluation of AI-driven learning tools. While other AI education symposiums exist, few meaningfully involve end-users from under-resourced contexts in the evaluation process. This symposium fills that gap by using a participatory model that integrates developers, rural educators, and remote learners into a structured, compensated evaluation framework.

The program features two tracks:
- **Real-World Impact Track**: For solutions that are ready or near-ready for deployment in low-resource settings.
- **Innovation Track**: For novel and experimental approaches.

Submitted prototypes will undergo structured testing by both expert educators from rural settings and remote learners from marginalized communities. Feedback from both panels will directly contribute to scoring, based on a transparent rubric that emphasizes technical innovation, cultural sensitivity, usability, ethical design, and scalability.

By embedding end-user perspectives into the evaluation process, the symposium pilots a replicable model for participatory, ethically grounded AI assessment. This initiative democratizes AI education innovation, elevates community expertise, and provides student developers with substantive, real-world validation. All materials, evaluation criteria, and recorded presentations will be made publicly available to maximize the broader impact.

## üèÜ Competition Tracks

||||
| -- | -- | -- |
| Track 1: üå±  Real-World Impact | For solutions ready (or near-ready) for deployment in under-resourced contexts | Winner: $1000 <br> Runner-up: $300 |
| Track 2: üí° Innovation |  For novel, experimental, or breakthrough approaches advancing ethical AI in remote learning | Winner: $700 <br> Runner-up: $200 |

## How it works?
- Submit your prototype and documentation: show how your AI solution works and why it matters
- Receive feedback from educators and remote learners: structured usability testing and surveys inform your final score
- Present at the live online showcase: finalists share their work, reflect with panels, and compete for awards

By participating, you‚Äôre helping shape AI that is responsible, usable, and truly equitable ‚Äî and getting recognized for it.

--- 

## Program Timeline

| Phase | Timeline | Tasks |
| :-- | :-- | :-- |
| 1. Submission üöÄ | May 6 ‚Äì July 6 | Participants submit: <br> - üé• 5‚Äì7 minute video demonstration <br> - üìÑ Technical + ethical documentation <br> - üß™ Functional prototype* | 
| 2. Participatory Prototype Evaluation üîé | July 7 ‚Äì July 28 | All projects undergo structured, community-informed evaluation |
| 3. Live Online Showcase & final judging üé§ | August 15 | Four-hour online symposium: <br> - Finalist presentations  <br> - Moderated Q&A  <br> - Reflections from educator and learner panelists <br>  - Community voting (non-determinative) <br>  - Awards ceremony  |

*Examples of prototypes: web app, mobile app, lightweight executable, or guided simulation hosted on an open repository 

### Panel 1: Expert educators
Educators from rural or under-resourced contexts evaluate:
- Feasibility  
- Pedagogical value  
- Cultural appropriateness  
- Ethical design  

### Panel 2: Remote learners 
Learners from marginalized communities participate in a 14-day guided testing period. They assess:
- Usability  
- Accessibility in low-bandwidth settings  
- Cultural resonance  
- Relevance to lived learning needs  

Participation includes:
- 2‚Äì3 hours total engagement  
- Structured surveys  
- Facilitated feedback sessions  
- Fair compensation  
- Privacy and risk safeguards  

## Scoring rubric

<details>
<summary> Needs Alignment (25%)</summary>
Description: Does the AI tool directly address the specific needs of marginalized or rural learners, particularly in remote and under-resourced contexts?

Questions to consider:
1. Does the tool solve a clearly identified problem in these communities (e.g., access to resources, low bandwidth, cultural relevance)?
2. How well does the solution align with the real-world needs of the target user group?
3. Has the project incorporated user feedback from remote learners and educators during the design phase?
4. Is there evidence of how users will be motivated to use the tool regularly (e.g., gamification, ease of use, feedback loops)?
</details>

<details>
<summary> Ethical Design (50%)</summary>
 
Description: Does the AI solution address potential biases, ensuring that it does not disadvantage certain groups, particularly marginalized communities?

Questions to consider:
### Fairness and Bias Mitigation
- Does the tool account for diverse learners and their needs (e.g., language, cultural backgrounds, socio-economic status)?
- How does the system identify and mitigate potential bias in its algorithms and data usage?
- Is there any evidence of fairness testing (e.g., does the tool work equally well for learners from different backgrounds)?
### Transparency & Accountability
- Does the tool provide clear explanations about its algorithms, data sources, and how decisions are made?
- Are the creators accountable for potential harm caused by the tool, and is there a process for addressing user concerns?
- Is there a clear and transparent methodology for how the tool was designed, tested, and iterated?
### Data Privacy & Security
- Does the tool follow ethical data collection practices, such as informed consent, data anonymization, and user control over their data?
- How does the tool ensure security for vulnerable populations, protecting them from data breaches or misuse?
- Does the tool comply with relevant data protection laws and local regulations for remote learners?
### Cultural Sensitivity and Inclusivity    
- Is the tool designed with cultural sensitivity in mind, addressing the unique needs and backgrounds of remote learners from diverse communities?
- How inclusive is the solution? Does it consider disabilities, language barriers, and regional diversity in design?
- Has the tool been tested with target communities to ensure cultural appropriateness?
</details>

<details>
<summary> Usability & Accessibility (25%)</summary>
Description: How intuitive and easy-to-use is the AI tool for learners, especially those with limited technical experience? Does the interface design facilitate smooth interaction?

Questions to consider:
1. Is the interface clear, intuitive, and easy to navigate without prior training or significant guidance?
2. Does the tool provide easy-to-understand instructions and feedback during use?
3. Is there a balance between simplicity (for accessibility) and functionality (for depth)?

Learners will be given a survey questions, e.g.:
- ‚ÄúI could use this tool without external help.‚Äù (1‚Äì5 scale)
- ‚ÄúThe tool worked well with limited internet.‚Äù (1‚Äì5 scale)
- ‚ÄúInstructions were clear.‚Äù (1‚Äì5 scale)
- ‚ÄúI would use this for my own learning.‚Äù (Yes/No + Why)

Educators then provide evaluation based on:
- Learners' ratings
- Thematic summary of learners' qualitative responses
- Flagged issues (e.g., confusion, harm concerns)
</details>
