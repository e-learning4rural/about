 

*An international showcase of ethical, community-evaluated AI solutions for remote and marginalized education contexts*


## ğŸŒŸ What this symposium is about

The Inaugural Symposium on Ethical AI Solutions for Remote Learning is a new online initiative that advances educational equity by centering the perspectives of rural and marginalized communities in the evaluation of AI-driven learning tools. While other AI education symposiums exist, few meaningfully involve end-users from under-resourced contexts in the evaluation process. This symposium fills that gap by using a participatory model that integrates developers, rural educators, and remote learners into a structured, compensated evaluation framework.

The program features two tracks:
- Real-World Impact Track: For solutions that are ready or near-ready for deployment in low-resource settings.
- Innovation Track: For novel and experimental approaches.

Submitted prototypes will undergo structured testing by both expert educators from rural settings and remote learners from marginalized communities. Feedback from both panels will directly contribute to scoring, based on a transparent rubric that emphasizes technical innovation, cultural sensitivity, usability, ethical design, and scalability.

By embedding end-user perspectives into the evaluation process, the symposium pilots a replicable model for participatory, ethically grounded AI assessment. This initiative democratizes AI education innovation, elevates community expertise, and provides student developers with substantive, real-world validation. All materials, evaluation criteria, and recorded presentations will be made publicly available to maximize the broader impact.

## ğŸ† Competition Tracks

||||
| -- | -- | -- |
| Track 1: ğŸŒ±  Real-World Impact | For solutions ready (or near-ready) for deployment in under-resourced contexts | Winner: $1000 <br> Runner-up: $300 |
| Track 2: ğŸ’¡ Innovation |  For novel, experimental, or breakthrough approaches advancing ethical AI in remote learning | Winner: $700 <br> Runner-up: $200 |

## How it works?
- Submit your prototype and documentation: show how your AI solution works and why it matters
- Receive feedback from educators and remote learners: structured usability testing and surveys inform your final score
- Present at the live online showcase: finalists share their work, reflect with panels, and compete for awards

By participating, youâ€™re helping shape AI that is responsible, usable, and truly equitable â€” and getting recognized for it.

--- 

## Program Timeline

| Phase | Timeline | Tasks |
| :-- | :-- | :-- |
| 1. Submission ğŸš€ | May 6 â€“ July 6 | Participants submit: <br> - ğŸ¥ 5â€“7 minute video demonstration <br> - ğŸ“„ Technical + ethical documentation <br> - ğŸ§ª Functional prototype* | 
| 2. Participatory Prototype Evaluation ğŸ” | July 7 â€“ July 28 | All projects undergo structured, community-informed evaluation |
|  3. Live Online Showcase & final judging ğŸ¤ | August 15 | Four-hour online symposium: <br> - Finalist presentations  <br> - Moderated Q&A  <br> - Reflections from educator and learner panelists <br>  - Community voting (non-determinative) <br>  - Awards ceremony  |

*Examples of prototypes: web app, mobile app, lightweight executable, or guided simulation hosted on an open repository 

### Panel 1: Expert educators
Educators from rural or under-resourced contexts evaluate:
- Feasibility  
- Pedagogical value  
- Cultural appropriateness  
- Ethical design  

### Panel 2: Remote learners 
Learners from marginalized communities participate in a 14-day guided testing period. They assess:
- Usability  
- Accessibility in low-bandwidth settings  
- Cultural resonance  
- Relevance to lived learning needs  

Participation includes:
- 2â€“3 hours total engagement  
- Structured surveys  
- Facilitated feedback sessions  
- Fair compensation  
- Privacy and risk safeguards  

 

<details>
  <summary> Rubric Category: Usability & Accessibility </summary>

Example of the learner survey items:
- â€œI could use this tool without external help.â€ (1â€“5 scale)
- â€œThe tool worked well with limited internet.â€ (1â€“5 scale)
- â€œInstructions were clear.â€ (1â€“5 scale)
- â€œI would use this for my own learning.â€ (Yes/No + Why)

Educators receive then provide evaluation:
- Quantitative averages
- Thematic summary of qualitative responses
- Flagged issues (e.g., confusion, harm concerns)
- Score â€œUsability & Accessibilityâ€ informed by:
  - Their professional judgment
  - Learner experience data
 
</details> 
